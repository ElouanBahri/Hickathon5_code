{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import os\n",
    "import s3fs\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, GRU, Dropout, Bidirectional, BatchNormalization\n",
    "from tensorflow.keras.activations import linear, relu, sigmoid\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from modules.utils import pre_process_data, encoded_categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_ENDPOINT_URL = \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})\n",
    "BUCKET = \"ebahri-ensae\"\n",
    "FILE_KEY_S3 = \"X_train_Hi5.csv\"\n",
    "FILE_PATH_S3 = BUCKET + \"/\" + FILE_KEY_S3\n",
    "\n",
    "with fs.open(FILE_PATH_S3, mode=\"rb\") as file_in:\n",
    "    x_train = pd.read_csv(file_in, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trained_without_nan, y_train = pre_process_data(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trained_without_nan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_rows', None)     # Show all rows\n",
    "pd.set_option('display.width', None)        # No line wrap\n",
    "pd.set_option('display.max_colwidth', None) # No truncation of columns\n",
    "\n",
    "nan_percentage_5 = x_trained_without_nan.isna().mean() * 100\n",
    "\n",
    "# Display the percentage of NaN values per column\n",
    "print(nan_percentage_5.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_trained_without_nan[\"insee_%_agri\"] = x_trained_without_nan[\"insee_%_agri\"].replace(\n",
    "    {'N/A - division par 0': 0}  # Replace with 0 or any value you choose\n",
    ").astype(float)\n",
    "x_trained_without_nan[\"insee_med_living_level\"] = x_trained_without_nan[\"insee_med_living_level\"].replace(\n",
    "    {'N/A - résultat non disponible': 0}  # Replace with 0 or any value you choose\n",
    ").astype(float)\n",
    "x_trained_without_nan[\"insee_%_ind\"] = x_trained_without_nan[\"insee_%_ind\"].replace(\n",
    "    {'N/A - division par 0': 0}  # Replace with 0 or any value you choose\n",
    ").astype(float)\n",
    "x_trained_without_nan[\"insee_%_const\"] = x_trained_without_nan[\"insee_%_const\"].replace(\n",
    "    {'N/A - division par 0': 0}  # Replace with 0 or any value you choose\n",
    ").astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final = encoded_categorical_features(x_trained_without_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final = pd.DataFrame(X_final)\n",
    "X_final.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final = X_final.fillna(X_final.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(X_final.corr(), annot=False, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((X_final,y_train ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of samples in your dataset\n",
    "dataset_size = len(X_final)\n",
    "train_size = int(0.6 * dataset_size)  # 60% for training\n",
    "val_size = int(0.2 * dataset_size)    # 20% for validation\n",
    "test_size = dataset_size - train_size - val_size  # Remaining 20% for testing\n",
    "\n",
    "# Shuffle the dataset\n",
    "dataset = dataset.shuffle(buffer_size=dataset_size)\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_dataset = dataset.take(train_size)    # First 60%\n",
    "val_dataset = dataset.skip(train_size).take(val_size)  # Next 20%\n",
    "test_dataset = dataset.skip(train_size + val_size)  # Remaining 20%\n",
    "\n",
    "# Batch the datasets\n",
    "batch_size = 32\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    # Input Layer with 89 features\n",
    "    Dense(64, activation='relu', input_shape=(89,)),  # Start with 64 neurons\n",
    "\n",
    "    # Output Layer: Softmax activation for classification (assuming 5 classes)\n",
    "    Dense(5, activation='softmax')  # Output layer for 5 classes\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    y_pred = K.argmax(y_pred, axis=-1)\n",
    "    true_positives = K.sum(K.cast(K.equal(y_true, y_pred), K.floatx()))\n",
    "    possible_positives = K.sum(K.cast(K.not_equal(y_true, 0), K.floatx()))  # assuming 0 is background\n",
    "    return true_positives / (K.sum(K.cast(K.equal(y_true, y_pred), K.floatx())) + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',  # Choose the metric to monitor\n",
    "    factor=0.5,          # Factor by which to reduce the learning rate\n",
    "    patience=3,          # Number of epochs with no improvement before reducing\n",
    "    min_lr=1e-6         # Minimum learning rate\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    '../models/first_model_3.keras',             # File to save the best model\n",
    "    monitor='val_accuracy',       # Metric to monitor for improvement\n",
    "    mode='max',                   # Mode 'max' for accuracy (since higher is better)\n",
    "    save_best_only=True,          # Save only when there is an improvement\n",
    "    verbose=1,                    # Print message when saving\n",
    "                    \n",
    ")\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data = val_dataset,\n",
    "    epochs=10,\n",
    "    callbacks=[reduce_lr, checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 1. Make predictions on the validation dataset\n",
    "y_pred = model.predict(val_dataset)\n",
    "\n",
    "# 2. Convert predictions to class labels if necessary\n",
    "# If the model's last layer is 'softmax', use np.argmax to get class labels\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# 3. Get the true labels from the validation dataset\n",
    "# Note: This assumes val_dataset is a tf.data.Dataset object containing (features, labels)\n",
    "y_true = np.concatenate([y for _, y in val_dataset], axis=0)\n",
    "\n",
    "# 4. Calculate the F1 score\n",
    "f1 = f1_score(y_true, y_pred_classes, average='weighted')  # 'weighted' is typically used for imbalanced classes\n",
    "\n",
    "# Print the F1 score\n",
    "print(f'F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score \n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "mon_X_train, mon_X_test, mon_y_train, mon_y_test = train_test_split(X_final,y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardiser les données\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(mon_X_train)\n",
    "X_test_scaled = scaler.transform(mon_X_test)\n",
    "\n",
    "# Initialiser le modèle XGBoost\n",
    "model = XGBClassifier(\n",
    "    objective=\"multi:softmax\",\n",
    "    num_class=5,\n",
    "    random_state=42,\n",
    "    eval_metric=\"mlogloss\",\n",
    "    colsample_bytree=0.7,\n",
    "    learning_rate=0.2,\n",
    "    max_depth=7,\n",
    "    n_estimators=500,\n",
    "    reg_alpha=0.5,\n",
    "    reg_lambda=0.0,\n",
    "    subsample=1.0\n",
    ")\n",
    "\n",
    "# Entraîner le modèle\n",
    "model.fit(X_train_scaled, mon_y_train)\n",
    "\n",
    "# Prédire sur le jeu de test\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Calculer le F1-score pondéré\n",
    "f1 = f1_score(mon_y_test, y_pred, average='weighted')\n",
    "print(f\"F1-Score pondéré sur le jeu de test : {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"xgboost_model_2.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hickathon5_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
