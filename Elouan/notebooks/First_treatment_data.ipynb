{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import s3fs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from modules.utils import complete_nan_meteo, complete_nan_prev, create_region, complete_nan_meteo_region, complete_nan_national"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_ENDPOINT_URL = \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})\n",
    "BUCKET = \"ebahri-ensae\"\n",
    "FILE_KEY_S3 = \"X_train_Hi5.csv\"\n",
    "FILE_PATH_S3 = BUCKET + \"/\" + FILE_KEY_S3\n",
    "\n",
    "with fs.open(FILE_PATH_S3, mode=\"rb\") as file_in:\n",
    "    x_train = pd.read_csv(file_in, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS_TO_DROP = [\n",
    "    \"piezo_station_department_name\",\n",
    "    \"piezo_station_commune_code_insee\",\n",
    "    \"piezo_station_bss_code\",\n",
    "    \"piezo_station_commune_name\",\n",
    "    \"piezo_bss_code\",\n",
    "    \"piezo_continuity_name\",\n",
    "    \"piezo_producer_code\",\n",
    "    \"piezo_producer_name\",\n",
    "    \"piezo_measure_nature_name\",\n",
    "    \"meteo_longitude\",\n",
    "    \"meteo_latitude\",\n",
    "    \"hydro_observation_date_elab\",\n",
    "    \"hydro_status_label\",\n",
    "    \"hydro_method_label\",\n",
    "    \"hydro_qualification_label\",\n",
    "    \"hydro_longitude\",\n",
    "    \"hydro_latitude\",\n",
    "    \"prelev_longitude_0\",\n",
    "    \"prelev_latitude_0\",\n",
    "    \"prelev_commune_code_insee_0\",\n",
    "    \"prelev_longitude_1\",\n",
    "    \"prelev_latitude_1\",\n",
    "    \"prelev_commune_code_insee_1\",\n",
    "    \"prelev_longitude_2\",\n",
    "    \"prelev_latitude_2\",\n",
    "    \"prelev_commune_code_insee_2\",\n",
    "    \"prelev_structure_code_2\",            \n",
    "    \"prelev_structure_code_1\",\n",
    "    \"prelev_structure_code_0\"\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.drop(columns=COLUMNS_TO_DROP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with more than 50% NaN values\n",
    "threshold = 0.9  # 50% threshold\n",
    "columns_to_drop = x_train.columns[x_train.isnull().mean() > threshold]\n",
    "\n",
    "x_train_cleaned = x_train.drop(columns=columns_to_drop)\n",
    "\n",
    "print(f\"Dropped columns: {columns_to_drop.tolist()}\")\n",
    "print(\"number\",len(columns_to_drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_rows', None)     # Show all rows\n",
    "pd.set_option('display.width', None)        # No line wrap\n",
    "pd.set_option('display.max_colwidth', None) # No truncation of columns\n",
    "\n",
    "nan_percentage = x_train_cleaned.isna().mean() * 100\n",
    "\n",
    "# Display the percentage of NaN values per column\n",
    "print(nan_percentage.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_completed=complete_nan_meteo(x_train_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_rows', None)     # Show all rows\n",
    "pd.set_option('display.width', None)        # No line wrap\n",
    "pd.set_option('display.max_colwidth', None) # No truncation of columns\n",
    "\n",
    "nan_percentage_1 = x_train_completed.isna().mean() * 100\n",
    "\n",
    "# Display the percentage of NaN values per column\n",
    "print(nan_percentage_1.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_completed_2 = complete_nan_prev(x_train_completed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_rows', None)     # Show all rows\n",
    "pd.set_option('display.width', None)        # No line wrap\n",
    "pd.set_option('display.max_colwidth', None) # No truncation of columns\n",
    "\n",
    "nan_percentage_2 = x_train_completed_2.isna().mean() * 100\n",
    "\n",
    "# Display the percentage of NaN values per column\n",
    "print(nan_percentage_2.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_3 = create_region(x_train_completed_2)\n",
    "print(x_train_3[\"region\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_4 = complete_nan_meteo_region(x_train_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_rows', None)     # Show all rows\n",
    "pd.set_option('display.width', None)        # No line wrap\n",
    "pd.set_option('display.max_colwidth', None) # No truncation of columns\n",
    "\n",
    "nan_percentage_4 = x_train_4.isna().mean() * 100\n",
    "\n",
    "# Display the percentage of NaN values per column\n",
    "print(nan_percentage_4.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_5 = complete_nan_national(x_train_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_rows', None)     # Show all rows\n",
    "pd.set_option('display.width', None)        # No line wrap\n",
    "pd.set_option('display.max_colwidth', None) # No truncation of columns\n",
    "\n",
    "nan_percentage_5 = x_train_5.isna().mean() * 100\n",
    "\n",
    "# Display the percentage of NaN values per column\n",
    "print(nan_percentage_5.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate numeric columns\n",
    "#numeric_cols = x_train.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Separate categorical columns\n",
    "categorical_cols = x_train_5.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Separate datetime columns\n",
    "#datetime_cols = x_train.select_dtypes(include=['datetime']).columns\n",
    "\n",
    "#print(\"Numeric Columns:\", numeric_cols)\n",
    "print(\"Categorical Columns:\", categorical_cols)\n",
    "#print(\"Datetime Columns:\", datetime_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attention biasÃ© si trop de NAN !!!!!!!\n",
    "\n",
    "\n",
    "#corr_matrix = x_train[numeric_cols].corr().abs()\n",
    "\n",
    "# Select the upper triangle of the correlation matrix\n",
    "#upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find features with correlation above the threshold (e.g., 0.7)\n",
    "#to_drop = [column for column in upper.columns if any(upper[column] > 0.7)]\n",
    "\n",
    "# Drop the columns from the dataset\n",
    "#x_train_dropped = x_train.drop(columns=to_drop)\n",
    "\n",
    "#print(f\"Dropped columns: {to_drop}\")\n",
    "#print(\"number\",len(to_drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Apply Label Encoding\n",
    "#label_encoders = {}\n",
    "#for col in categorical_cols:\n",
    "    #le = LabelEncoder()\n",
    "    #x_train[col] = le.fit_transform(x_train[col].astype(str))  # Convert to str to handle NaN\n",
    "    #label_encoders[col] = le  # Save the encoder for later use"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hickathon5_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
